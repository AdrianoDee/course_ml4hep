{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost and SUSY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Overview\n",
    "\n",
    "In this notebook, we will focus on using Gradient Boosted Trees (in particular XGBoost) to classify the supersymmetry (SUSY) dataset, first introduced by [Baldi et al. Nature Communication 2015 and Arxiv:1402.4735](https://arxiv.org/pdf/1402.4735.pdf). The supersymmetry data set consists of 5,000,000 Monte-Carlo samples of supersymmetric and non-supersymmetric events (with 18 features) coming from a dummy high-energy particle collider experiments. The *signal process* is the production of electrically-charged\n",
    "supersymmetric particles which decay to W bosons and an electrically-neutral supersymmetric\n",
    "particle that is invisible to the detector. \n",
    "\n",
    "\n",
    "![alt text](images/susy_sig.png \"Title\") \n",
    "\n",
    "The background process is the production of pairs of W bosons, which decay to charged leptons l and invisible neutrinos Î½.\n",
    "\n",
    "![alt text](images/susy_bkg.png \"Title\") \n",
    "\n",
    "The first 8 features are \"raw\" (low-level) kinematic features that can be directly measured from collisions (e.g. lepton $p_T$). The final 10 features are \"hand constructed\" (high-level) features that have been chosen using physical knowledge, and are known to be important in distinguishing supersymmetric and non-supersymmetric collision events. More specifically, they are (from the paper):\n",
    "\n",
    "<img src=\"images/susy_vars.png\"  width=\"60%\">\n",
    "\n",
    "As we will see (and have seen), there are many practical trade-offs we have to worry about. Unlike Random Forests, for more complicated algorithms such as XGBoost, overfitting can be a major worry. It is also extremely computationally expensive to do hyperparameter searches.\n",
    "\n",
    "#### Downloading the SUSY dataset\n",
    "The supersymmetry dataset can be downloaded from the UCI Machine Learning repository on [https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz](https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz). The dataset is quite large (~2GB). *Download the dataset and unzip it in a directory*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  879M  100  879M    0     0  17.4M      0  0:00:50  0:00:50 --:--:-- 18.9M-- --:--:-- --:--:--     0:36 18.1M\n"
     ]
    }
   ],
   "source": [
    "!curl http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz -o SUSY.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dataset and construct training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset : 100000\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset using pandas and numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filename [CHANGE THIS TO YOUR FILENAME FOR SUSY]\n",
    "filename='susy_data/SUSY.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in SUSY File. We will only work with subset of data for demonstration purposes.\n",
    "\n",
    "features=['SUSY','lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n",
    " 'missing energy magnitude', 'missing energy phi', 'MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2', \n",
    " 'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']\n",
    "\n",
    "low_features=['lepton 1 pT', 'lepton 1 eta', 'lepton 1 phi', 'lepton 2 pT', 'lepton 2 eta', 'lepton 2 phi', \n",
    " 'missing energy magnitude', 'missing energy phi']\n",
    "\n",
    "high_features=['MET_rel', 'axial MET', 'M_R', 'M_TR_2', 'R', 'MT2','S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset : 100000\n"
     ]
    }
   ],
   "source": [
    "#Number of datapoints to work with, limiting to 100k for the moment\n",
    "N = 100000\n",
    "print(\"Size of dataset : %i\"%N)\n",
    "df = pd.read_csv(filename, header=None,nrows=N,engine='python')\n",
    "df.columns=features\n",
    "y = df['SUSY'].values\n",
    "X = df[[col for col in df.columns if col!=\"SUSY\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make datasets using only the 8 low-level features and 10 high-level features\n",
    "X_low=X[low_features]\n",
    "X_high=X[high_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=0)\n",
    "X_low_train, X_low_test, y_low_train, y_low_test = train_test_split(X_low, y, test_size=.1, random_state=0)\n",
    "X_high_train, X_high_test, y_high_train, y_high_test = train_test_split(X_high, y, test_size=.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"susy_data/susy_small.pkl\",compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUSY</th>\n",
       "      <th>lepton 1 pT</th>\n",
       "      <th>lepton 1 eta</th>\n",
       "      <th>lepton 1 phi</th>\n",
       "      <th>lepton 2 pT</th>\n",
       "      <th>lepton 2 eta</th>\n",
       "      <th>lepton 2 phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>MET_rel</th>\n",
       "      <th>axial MET</th>\n",
       "      <th>M_R</th>\n",
       "      <th>M_TR_2</th>\n",
       "      <th>R</th>\n",
       "      <th>MT2</th>\n",
       "      <th>S_R</th>\n",
       "      <th>M_Delta_R</th>\n",
       "      <th>dPhi_r_b</th>\n",
       "      <th>cos(theta_r1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972861</td>\n",
       "      <td>0.653855</td>\n",
       "      <td>1.176225</td>\n",
       "      <td>1.157156</td>\n",
       "      <td>-1.739873</td>\n",
       "      <td>-0.874309</td>\n",
       "      <td>0.567765</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.810061</td>\n",
       "      <td>-0.252552</td>\n",
       "      <td>1.921887</td>\n",
       "      <td>0.889637</td>\n",
       "      <td>0.410772</td>\n",
       "      <td>1.145621</td>\n",
       "      <td>1.932632</td>\n",
       "      <td>0.994464</td>\n",
       "      <td>1.367815</td>\n",
       "      <td>0.040714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.667973</td>\n",
       "      <td>0.064191</td>\n",
       "      <td>-1.225171</td>\n",
       "      <td>0.506102</td>\n",
       "      <td>-0.338939</td>\n",
       "      <td>1.672543</td>\n",
       "      <td>3.475464</td>\n",
       "      <td>-1.219136</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>3.775174</td>\n",
       "      <td>1.045977</td>\n",
       "      <td>0.568051</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448410</td>\n",
       "      <td>0.205356</td>\n",
       "      <td>1.321893</td>\n",
       "      <td>0.377584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444840</td>\n",
       "      <td>-0.134298</td>\n",
       "      <td>-0.709972</td>\n",
       "      <td>0.451719</td>\n",
       "      <td>-1.613871</td>\n",
       "      <td>-0.768661</td>\n",
       "      <td>1.219918</td>\n",
       "      <td>0.504026</td>\n",
       "      <td>1.831248</td>\n",
       "      <td>-0.431385</td>\n",
       "      <td>0.526283</td>\n",
       "      <td>0.941514</td>\n",
       "      <td>1.587535</td>\n",
       "      <td>2.024308</td>\n",
       "      <td>0.603498</td>\n",
       "      <td>1.562374</td>\n",
       "      <td>1.135454</td>\n",
       "      <td>0.180910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381256</td>\n",
       "      <td>-0.976145</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>0.448959</td>\n",
       "      <td>0.891753</td>\n",
       "      <td>-0.677328</td>\n",
       "      <td>2.033060</td>\n",
       "      <td>1.533041</td>\n",
       "      <td>3.046260</td>\n",
       "      <td>-1.005285</td>\n",
       "      <td>0.569386</td>\n",
       "      <td>1.015211</td>\n",
       "      <td>1.582217</td>\n",
       "      <td>1.551914</td>\n",
       "      <td>0.761215</td>\n",
       "      <td>1.715464</td>\n",
       "      <td>1.492257</td>\n",
       "      <td>0.090719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.309996</td>\n",
       "      <td>-0.690089</td>\n",
       "      <td>-0.676259</td>\n",
       "      <td>1.589283</td>\n",
       "      <td>-0.693326</td>\n",
       "      <td>0.622907</td>\n",
       "      <td>1.087562</td>\n",
       "      <td>-0.381742</td>\n",
       "      <td>0.589204</td>\n",
       "      <td>1.365479</td>\n",
       "      <td>1.179295</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.728563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083158</td>\n",
       "      <td>0.043429</td>\n",
       "      <td>1.154854</td>\n",
       "      <td>0.094859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUSY  lepton 1 pT  lepton 1 eta  lepton 1 phi  lepton 2 pT  lepton 2 eta  \\\n",
       "0   0.0     0.972861      0.653855      1.176225     1.157156     -1.739873   \n",
       "1   1.0     1.667973      0.064191     -1.225171     0.506102     -0.338939   \n",
       "2   1.0     0.444840     -0.134298     -0.709972     0.451719     -1.613871   \n",
       "3   1.0     0.381256     -0.976145      0.693152     0.448959      0.891753   \n",
       "4   1.0     1.309996     -0.690089     -0.676259     1.589283     -0.693326   \n",
       "\n",
       "   lepton 2 phi  missing energy magnitude  missing energy phi   MET_rel  \\\n",
       "0     -0.874309                  0.567765           -0.175000  0.810061   \n",
       "1      1.672543                  3.475464           -1.219136  0.012955   \n",
       "2     -0.768661                  1.219918            0.504026  1.831248   \n",
       "3     -0.677328                  2.033060            1.533041  3.046260   \n",
       "4      0.622907                  1.087562           -0.381742  0.589204   \n",
       "\n",
       "   axial MET       M_R    M_TR_2         R       MT2       S_R  M_Delta_R  \\\n",
       "0  -0.252552  1.921887  0.889637  0.410772  1.145621  1.932632   0.994464   \n",
       "1   3.775174  1.045977  0.568051  0.481928  0.000000  0.448410   0.205356   \n",
       "2  -0.431385  0.526283  0.941514  1.587535  2.024308  0.603498   1.562374   \n",
       "3  -1.005285  0.569386  1.015211  1.582217  1.551914  0.761215   1.715464   \n",
       "4   1.365479  1.179295  0.968218  0.728563  0.000000  1.083158   0.043429   \n",
       "\n",
       "   dPhi_r_b  cos(theta_r1)  \n",
       "0  1.367815       0.040714  \n",
       "1  1.321893       0.377584  \n",
       "2  1.135454       0.180910  \n",
       "3  1.492257       0.090719  \n",
       "4  1.154854       0.094859  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8z/l26418z13k77svsgfsmn9vj40000gp/T/ipykernel_78564/583325184.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# For next cell\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/adrianodif/miniconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0           8 KB\n",
      "    joblib-1.1.1               |   py39hecd8cb5_0         384 KB\n",
      "    libcxx-14.0.6              |       h9765a3e_0         968 KB\n",
      "    libxgboost-1.7.3           |       hcec6c5f_0         2.3 MB\n",
      "    llvm-openmp-14.0.6         |       h0dcd299_0         288 KB\n",
      "    py-xgboost-1.7.3           |   py39hecd8cb5_0         216 KB\n",
      "    scikit-learn-1.2.2         |   py39hcec6c5f_0         7.0 MB\n",
      "    threadpoolctl-2.2.0        |     pyh0d69192_0          16 KB\n",
      "    xgboost-1.7.3              |   py39hecd8cb5_0          14 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        11.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  pkgs/main/osx-64::_py-xgboost-mutex-2.0-cpu_0 None\n",
      "  joblib             pkgs/main/osx-64::joblib-1.1.1-py39hecd8cb5_0 None\n",
      "  libxgboost         pkgs/main/osx-64::libxgboost-1.7.3-hcec6c5f_0 None\n",
      "  py-xgboost         pkgs/main/osx-64::py-xgboost-1.7.3-py39hecd8cb5_0 None\n",
      "  scikit-learn       pkgs/main/osx-64::scikit-learn-1.2.2-py39hcec6c5f_0 None\n",
      "  threadpoolctl      pkgs/main/noarch::threadpoolctl-2.2.0-pyh0d69192_0 None\n",
      "  xgboost            pkgs/main/osx-64::xgboost-1.7.3-py39hecd8cb5_0 None\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  libcxx                                           10.0.0-1 --> 14.0.6-h9765a3e_0 None\n",
      "  llvm-openmp                             12.0.0-h0dcd299_1 --> 14.0.6-h0dcd299_0 None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \n",
      "libcxx-14.0.6        | 968 KB    | ##################################### | 100% \n",
      "llvm-openmp-14.0.6   | 288 KB    | ##################################### | 100% \n",
      "scikit-learn-1.2.2   | 7.0 MB    | ##################################### | 100% \n",
      "joblib-1.1.1         | 384 KB    | ##################################### | 100% \n",
      "py-xgboost-1.7.3     | 216 KB    | ##################################### | 100% \n",
      "xgboost-1.7.3        | 14 KB     | ##################################### | 100% \n",
      "threadpoolctl-2.2.0  | 16 KB     | ##################################### | 100% \n",
      "libxgboost-1.7.3     | 2.3 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: | \n",
      "\n",
      "    Installed package of scikit-learn can be accelerated using scikit-learn-intelex.\n",
      "    More details are available here: https://intel.github.io/scikit-learn-intelex\n",
      "\n",
      "    For example:\n",
      "\n",
      "        $ conda install scikit-learn-intelex\n",
      "        $ python -m sklearnex my_application.py\n",
      "\n",
      "    \n",
      "\n",
      "done\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install xgboost -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "print(\"Training on %i examples with %i features\"%X_train.shape)\n",
    "\n",
    "#Use default parameters and train on full dataset\n",
    "XGBclassifier = xgb.sklearn.XGBClassifier(nthread=-1, seed=1, n_estimators=1000)\n",
    "#Train and time classifier\n",
    "start_time = time.time()\n",
    "XGBclassifier.fit(X_train, y_train)\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "#Make Predictions\n",
    "print(\"Predicting on %i examples with %i features\\n\"%X_test.shape)\n",
    "y_pred= XGBclassifier.predict(X_test)\n",
    "\n",
    "#Print Results\n",
    "print(\"Model Accuracy with all features: {:.2f}%\".format(100*XGBclassifier.score(X_test, y_test)))\n",
    "print(\"The AUC score with all features is {:.2f}\".format(roc_auc_score(y_test,y_pred)))\n",
    "print(\"Run time with all features: {:.2f} sec\\n\\n\".format(run_time))\n",
    "\n",
    "\n",
    "#Rerun with just low-level kinematic features with default parameters\n",
    "\n",
    "print(\"Training on %i examples with %i features\"%X_low_train.shape)\n",
    "XGBclassifier_low = xgb.sklearn.XGBClassifier(nthread=-1, seed=1)\n",
    "#Train and time classifier\n",
    "start_time = time.time()\n",
    "XGBclassifier_low.fit(X_low_train, y_low_train)\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "#Make Predictions\n",
    "print(\"Predicting on %i examples with %i features\\n\"%X_low_test.shape)\n",
    "y_low_pred = XGBclassifier_low.predict(X_low_test)\n",
    "\n",
    "#Print Results\n",
    "print(\"Model Accuracy with just low-level kinematic features: {:.2f}%\".format(100*XGBclassifier_low.score(X_low_test, y_low_test)))\n",
    "print(\"The low-level AUC score is {:.2f}\".format(roc_auc_score(y_test,y_low_pred)))\n",
    "print(\"Run time with low-level features: {:.2f} sec\\n\\n\".format(run_time))\n",
    "\n",
    "\n",
    "#Rerun with just high-level kinematic features with default parameters\n",
    "\n",
    "print(\"Training on %i examples with %i features\\n\"%X_high_train.shape)\n",
    "XGBclassifier_high = xgb.sklearn.XGBClassifier(nthread=-1, seed=1)\n",
    "#Train and time classifier\n",
    "start_time = time.time()\n",
    "XGBclassifier_high.fit(X_high_train, y_high_train)\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "print(\"Training on %i examples with %i features\"%X_high_test.shape)\n",
    "#Make Predictions\n",
    "y_high_pred = XGBclassifier_high.predict(X_high_test)\n",
    "\n",
    "#Print Results\n",
    "print(\"Model Accuracy with just high-level features: {:.2f}%\".format(100*XGBclassifier_low.score(X_low_test, y_low_test)))\n",
    "print(\"The high-level AUC score is {:.2f}\".format(roc_auc_score(y_test,y_high_pred)))\n",
    "print(\"Run time with high-level features: {:.2f} sec\\n\\n\".format(run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Feature Importance\n",
    "\n",
    "One nice aspect of XGBoost (and ensemble methods in general) is that it is easy to visualize feature importances. In XGBoost, there are some handy plots for viewing these (similar functions also exist for the scikit implementation of random forests). One thing we can calculate is the feature importance score (Fscore), which measures how many times each feature was split on. The higher this number, the more fine-tuned the partitions in this direction, and presumably the more informative it is for our classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8z/l26418z13k77svsgfsmn9vj40000gp/T/ipykernel_78564/2305739476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXGBclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#fig.savefig('SUSYXGBoost1.pdf')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import ml_style as style\n",
    "import matplotlib as mpl\n",
    "#mpl.rcParams.update(style.style)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "fig=plt.figure(figsize=(12,9))\n",
    "xgb.plot_importance(XGBclassifier, ax=plt.gca())\n",
    "fig.subplots_adjust(left=0.4)  #\n",
    "#fig.savefig('SUSYXGBoost1.pdf')\n",
    "\n",
    "fig=plt.figure(figsize=(12,9))\n",
    "xgb.plot_importance(XGBclassifier_low, ax=plt.gca())\n",
    "fig.subplots_adjust(left=0.4)\n",
    "#fig.savefig('SUSYXGBoost2.pdf')\n",
    "fig=plt.figure(figsize=(12,9))\n",
    "xgb.plot_importance(XGBclassifier_high, ax=plt.gca())\n",
    "fig.subplots_adjust(left=0.4)\n",
    "#fig.savefig('SUSYXGBoost3.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot ROC curves\n",
    "\n",
    "This simple example shows that with the default parameters one can already achieve an accuracy of about 80 percent using all the features (kinematic and hand crafted), and a slightly smaller accuracy of about 78.75% using just the kinematic features.  Both achieve a very respectable AUC (area under the ROC curve, see [https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)) score of around 0.78 or 0.79. This is significantly better than that achieved using Boosted Decision Trees (though not deep neural networks) in the original [paper](https://www.nature.com/articles/ncomms5308), even without tuning hyperparameters. Furthermore, we are using only a small subset of all the data (100,000 out of a total of 5,000,000 datapoints) so this performance is a lower bound on what can be accomplished with XGBoost. Note that there are only three points on the curve so the ROC does not contain much information beyond the accuracy.\n",
    "\n",
    "We can summarize this by plotting the ROC curves for these three models. Recall that ROC curves plot the true positive rate. Here, we will use the modified version used in high-energy physics plotting the true negative rate (Background rejection) against the true positive rate (signal efficiency).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "fpr_low, tpr_low, _ = roc_curve(y_test, y_low_pred)\n",
    "fpr_high, tpr_high, _ = roc_curve(y_test, y_high_pred)\n",
    "plt.figure(1)\n",
    "plt.plot(tpr, 1-fpr, label='Full')\n",
    "plt.plot(tpr_low, 1-fpr_low, label='Low')\n",
    "plt.plot(tpr_high, 1-fpr_high, label='High')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Signal efficiency')\n",
    "plt.ylabel('Background Rejection')\n",
    "plt.savefig(\"SUSY_roc_XGBoost.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing XGBoost\n",
    "\n",
    "We will now optimize the parameters of the XGBoost algorithm by performing a grid search. We will use the very useful new function from scikit-learn `GridSearchCV()`. This function allows you to specify lists of parameters to search over.\n",
    "\n",
    "Let us briefly discuss what parameters we can tune to improve performance with descriptions:\n",
    "\n",
    "* `max_depth` [default=6]: maximum depth of a tree, increasing this value will make the model more complex / likely to overfit.\n",
    "\n",
    "* `eta` or 'learning_rate'[default =0.3]: step size shrinkage used in update to prevent overfitting. After each boosting step, we can directly get the weights of new features. `eta` actually shrinks the feature weights to make the boosting process more conservative.\n",
    "\n",
    "* `gamma` or min-split-loss [default=0]: This is the penalty that regularizes the number of leaves. The larger, the more conservative the algorithm will be. \n",
    "\n",
    "* `min_child_weight` [default=1]: In linear regression mode, this simply corresponds to the minimum number of instances needed to be in each node (min $B_j$ in notation of manuscript). The larger, the more conservative the algorithm will be. More generally, it is the minimum sum of instance weight (Hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. \n",
    "\n",
    "\n",
    "As you can see this cross-validation procedure is quite computationally expensive. With the parameters below, it takes somewhere between 2 and 5 minutes on a powerful laptop. In the cell below, we perform the search and examine the results in the subsequent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "#Create values to search over\n",
    "cv_params = {'max_depth': [3,4,6], 'min_child_weight': [1,3,5], 'learning_rate':[0.1,0.3]}\n",
    "ind_params = {'n_estimators': 100, 'seed':1, 'colsample_bytree': 1, \n",
    "             'objective': 'binary:logistic'}\n",
    "opt_XGBclassifier = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1, verbose=3)\n",
    "\n",
    "opt_XGBclassifier.fit(X_train, y_train)\n",
    "opt_XGBclassifier.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal score on training set is 0.797\n",
      "The optimal parameters for the classifier are:\n",
      "{'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 3}\n",
      "Model Accuray with optimal parameters: 79.78%\n",
      "The  AUC score is 0.79\n"
     ]
    }
   ],
   "source": [
    "#Print scores\n",
    "print('The optimal score on training set is {:0.3f}'.format(opt_XGBclassifier.best_score_))\n",
    "\n",
    "#Find optimal parameters\n",
    "\n",
    "print('The optimal parameters for the classifier are:')\n",
    "print(opt_XGBclassifier.best_params_)\n",
    "\n",
    "#Fit performance on the test set\n",
    "XGBclassifier_final=opt_XGBclassifier.best_estimator_\n",
    "y_pred_final=XGBclassifier_final.predict(X_test)\n",
    "print(\"Model Accuray with optimal parameters: {:.2f}%\".format(100*XGBclassifier_final.score(X_test, y_test)))\n",
    "print(\"The  AUC score is {:.2f}\".format(roc_auc_score(y_test,y_pred_final)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Optimization: early stopping and computational efficiency\n",
    "\n",
    "We see that we have slightly improved our performance. The default parameters work pretty well. Here, we have used relatively small ensembles (100) to make things faster. In practice, it is worth using much bigger ensembles, in which case overfitting and optimization are likely to have larger effects.\n",
    "\n",
    "Following this very nice blog post, [https://jessesw.com/XG-Boost/](https://jessesw.com/XG-Boost/), we can do some further optimization of the XGBoost algorithm. Part of this is computational and has to do with how we interface with XGBoost, and part will be due to another regularization technique that we discussed in the gradient descent chapter: *early stopping*. Early stopping is now considered one of the most important regularization techniques. The basic idea behind it is to just stop the gradient descent once some measure of error stops going down significantly. \n",
    "\n",
    "You are invited to play with the code and experiment with this yourself.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
